{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home1/09059/xliaoyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home1/09059/xliaoyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home1/09059/xliaoyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, AutoModelForSequenceClassification, BertTokenizerFast, TFBertModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, TextClassificationPipeline, BertModel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import re, string\n",
    "import emoji\n",
    "import nltk\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "from datasets import Dataset , Sequence , Value , Features , ClassLabel , DatasetDict\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import re, string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Clean emojis from text\n",
    "def strip_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r\"\", text)\n",
    "\n",
    "#Remove punctuations, links, mentions and \\r\\n new line characters\n",
    "def strip_all_entities(text): \n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    return text\n",
    "\n",
    "#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n",
    "def clean_hashtags(tweet):\n",
    "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n",
    "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n",
    "    return new_tweet2\n",
    "\n",
    "#Filter special characters such as & and $ present in some words\n",
    "def filter_chars(a):\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)\n",
    "\n",
    "def remove_mult_spaces(text): # remove multiple spaces\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "df_train = pd.read_csv(\"Corona_NLP_train.csv\", \n",
    "                       encoding='latin-1')\n",
    "df_test = pd.read_csv(\"Corona_NLP_test.csv\", \n",
    "                      encoding='latin-1')\n",
    "\n",
    "df_train = df_train[['OriginalTweet', 'Sentiment']]\n",
    "df_test = df_test[['OriginalTweet', 'Sentiment']]\n",
    "\n",
    "# clean data\n",
    "\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_emoji)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_all_entities)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(clean_hashtags)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(filter_chars)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(remove_mult_spaces)\n",
    "\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_emoji)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_all_entities)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(clean_hashtags)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(filter_chars)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(remove_mult_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.5596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.47      0.54      1056\n",
      "           1       0.50      0.50      0.50      2006\n",
      "           2       0.60      0.66      0.63      1553\n",
      "           3       0.51      0.61      0.56      2287\n",
      "           4       0.68      0.53      0.60      1330\n",
      "\n",
      "    accuracy                           0.56      8232\n",
      "   macro avg       0.58      0.55      0.56      8232\n",
      "weighted avg       0.57      0.56      0.56      8232\n",
      "\n",
      "K-Nearest Neighbors - Accuracy: 0.2194\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.04      0.07      1056\n",
      "           1       0.46      0.07      0.13      2006\n",
      "           2       0.20      0.96      0.32      1553\n",
      "           3       0.59      0.04      0.08      2287\n",
      "           4       0.68      0.02      0.04      1330\n",
      "\n",
      "    accuracy                           0.22      8232\n",
      "   macro avg       0.51      0.23      0.13      8232\n",
      "weighted avg       0.51      0.22      0.13      8232\n",
      "\n",
      "SVM - Accuracy: 0.5788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.43      0.53      1056\n",
      "           1       0.51      0.57      0.54      2006\n",
      "           2       0.64      0.63      0.64      1553\n",
      "           3       0.52      0.68      0.59      2287\n",
      "           4       0.78      0.48      0.59      1330\n",
      "\n",
      "    accuracy                           0.58      8232\n",
      "   macro avg       0.63      0.56      0.58      8232\n",
      "weighted avg       0.61      0.58      0.58      8232\n",
      "\n",
      "Random Forest - Accuracy: 0.5237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.43      0.50      1056\n",
      "           1       0.49      0.46      0.47      2006\n",
      "           2       0.54      0.72      0.62      1553\n",
      "           3       0.48      0.55      0.51      2287\n",
      "           4       0.61      0.43      0.51      1330\n",
      "\n",
      "    accuracy                           0.52      8232\n",
      "   macro avg       0.54      0.52      0.52      8232\n",
      "weighted avg       0.53      0.52      0.52      8232\n",
      "\n",
      "Decision Tree - Accuracy: 0.4427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.38      0.40      1056\n",
      "           1       0.41      0.38      0.39      2006\n",
      "           2       0.48      0.59      0.53      1553\n",
      "           3       0.42      0.43      0.42      2287\n",
      "           4       0.49      0.45      0.47      1330\n",
      "\n",
      "    accuracy                           0.44      8232\n",
      "   macro avg       0.45      0.44      0.44      8232\n",
      "weighted avg       0.44      0.44      0.44      8232\n",
      "\n",
      "Naive Bayes - Accuracy: 0.4588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.21      0.32      1056\n",
      "           1       0.42      0.51      0.46      2006\n",
      "           2       0.60      0.39      0.48      1553\n",
      "           3       0.40      0.69      0.51      2287\n",
      "           4       0.65      0.27      0.38      1330\n",
      "\n",
      "    accuracy                           0.46      8232\n",
      "   macro avg       0.55      0.41      0.43      8232\n",
      "weighted avg       0.52      0.46      0.45      8232\n",
      "\n",
      "AdaBoost - Accuracy: 0.4368\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.35      0.44      1056\n",
      "           1       0.32      0.63      0.43      2006\n",
      "           2       0.50      0.38      0.43      1553\n",
      "           3       0.50      0.39      0.44      2287\n",
      "           4       0.69      0.36      0.47      1330\n",
      "\n",
      "    accuracy                           0.44      8232\n",
      "   macro avg       0.52      0.42      0.44      8232\n",
      "weighted avg       0.50      0.44      0.44      8232\n",
      "\n",
      "Gradient Boosting - Accuracy: 0.4648\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.40      0.47      1056\n",
      "           1       0.52      0.30      0.38      2006\n",
      "           2       0.51      0.49      0.50      1553\n",
      "           3       0.37      0.65      0.47      2287\n",
      "           4       0.64      0.42      0.51      1330\n",
      "\n",
      "    accuracy                           0.46      8232\n",
      "   macro avg       0.52      0.45      0.47      8232\n",
      "weighted avg       0.50      0.46      0.46      8232\n",
      "\n",
      "XGBoost - Accuracy: 0.5441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.47      0.52      1056\n",
      "           1       0.52      0.42      0.46      2006\n",
      "           2       0.55      0.72      0.62      1553\n",
      "           3       0.49      0.58      0.53      2287\n",
      "           4       0.67      0.53      0.59      1330\n",
      "\n",
      "    accuracy                           0.54      8232\n",
      "   macro avg       0.56      0.54      0.55      8232\n",
      "weighted avg       0.55      0.54      0.54      8232\n",
      "\n",
      "LightGBM - Accuracy: 0.5864\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.55      0.57      1056\n",
      "           1       0.55      0.45      0.49      2006\n",
      "           2       0.60      0.78      0.68      1553\n",
      "           3       0.55      0.58      0.57      2287\n",
      "           4       0.67      0.61      0.64      1330\n",
      "\n",
      "    accuracy                           0.59      8232\n",
      "   macro avg       0.59      0.59      0.59      8232\n",
      "weighted avg       0.59      0.59      0.58      8232\n",
      "\n",
      "CatBoost - Accuracy: 0.5951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.50      0.57      1056\n",
      "           1       0.55      0.50      0.52      2006\n",
      "           2       0.60      0.78      0.68      1553\n",
      "           3       0.56      0.64      0.60      2287\n",
      "           4       0.74      0.52      0.61      1330\n",
      "\n",
      "    accuracy                           0.60      8232\n",
      "   macro avg       0.62      0.59      0.59      8232\n",
      "weighted avg       0.60      0.60      0.59      8232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "df_train['Sentiment'] = df_train['Sentiment'].replace(['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'], [0, 1, 2, 3, 4])\n",
    "X = df_train['OriginalTweet'].values\n",
    "y = df_train['Sentiment'].values\n",
    "\n",
    "# Create a TF-IDF representation of the data\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2), max_features=10000)\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# All classification models from scikit-learn\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('SVM', SVC(random_state=42)),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Naive Bayes', MultinomialNB()),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(eval_metric='mlogloss', random_state=42, use_label_encoder=False)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=42)),\n",
    "    ('CatBoost', CatBoostClassifier(random_state=42, verbose=0))\n",
    "]\n",
    "\n",
    "# Evaluate each model\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'{name} - Accuracy: {accuracy:.4f}')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "df_train = pd.read_csv(\"Corona_NLP_train.csv\", \n",
    "                       encoding='latin-1')\n",
    "df_test = pd.read_csv(\"Corona_NLP_test.csv\", \n",
    "                      encoding='latin-1')\n",
    "\n",
    "df_train = df_train[['OriginalTweet', 'Sentiment']]\n",
    "df_test = df_test[['OriginalTweet', 'Sentiment']]\n",
    "\n",
    "# clean data\n",
    "\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_emoji)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_all_entities)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(clean_hashtags)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(filter_chars)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(remove_mult_spaces)\n",
    "\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_emoji)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_all_entities)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(clean_hashtags)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(filter_chars)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(remove_mult_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.6251\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.58      0.62      1056\n",
      "           1       0.57      0.55      0.56      2006\n",
      "           2       0.65      0.74      0.69      1553\n",
      "           3       0.59      0.62      0.61      2287\n",
      "           4       0.72      0.65      0.68      1330\n",
      "\n",
      "    accuracy                           0.63      8232\n",
      "   macro avg       0.64      0.63      0.63      8232\n",
      "weighted avg       0.63      0.63      0.62      8232\n",
      "\n",
      "K-Nearest Neighbors - Accuracy: 0.2719\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.13      0.20      1056\n",
      "           1       0.35      0.18      0.24      2006\n",
      "           2       0.23      0.88      0.36      1553\n",
      "           3       0.39      0.14      0.20      2287\n",
      "           4       0.74      0.04      0.07      1330\n",
      "\n",
      "    accuracy                           0.27      8232\n",
      "   macro avg       0.43      0.27      0.22      8232\n",
      "weighted avg       0.41      0.27      0.22      8232\n",
      "\n",
      "SVM - Accuracy: 0.5694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.40      0.50      1056\n",
      "           1       0.50      0.52      0.51      2006\n",
      "           2       0.61      0.69      0.65      1553\n",
      "           3       0.51      0.67      0.58      2287\n",
      "           4       0.79      0.48      0.59      1330\n",
      "\n",
      "    accuracy                           0.57      8232\n",
      "   macro avg       0.62      0.55      0.57      8232\n",
      "weighted avg       0.59      0.57      0.57      8232\n",
      "\n",
      "Random Forest - Accuracy: 0.5146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.26      0.38      1056\n",
      "           1       0.48      0.50      0.49      2006\n",
      "           2       0.56      0.69      0.62      1553\n",
      "           3       0.45      0.66      0.54      2287\n",
      "           4       0.77      0.29      0.42      1330\n",
      "\n",
      "    accuracy                           0.51      8232\n",
      "   macro avg       0.59      0.48      0.49      8232\n",
      "weighted avg       0.56      0.51      0.50      8232\n",
      "\n",
      "Decision Tree - Accuracy: 0.4580\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.38      0.40      1056\n",
      "           1       0.42      0.42      0.42      2006\n",
      "           2       0.50      0.58      0.53      1553\n",
      "           3       0.45      0.45      0.45      2287\n",
      "           4       0.52      0.45      0.48      1330\n",
      "\n",
      "    accuracy                           0.46      8232\n",
      "   macro avg       0.46      0.46      0.46      8232\n",
      "weighted avg       0.46      0.46      0.46      8232\n",
      "\n",
      "Naive Bayes - Accuracy: 0.4742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.30      0.40      1056\n",
      "           1       0.42      0.52      0.47      2006\n",
      "           2       0.71      0.34      0.46      1553\n",
      "           3       0.41      0.64      0.50      2287\n",
      "           4       0.59      0.42      0.49      1330\n",
      "\n",
      "    accuracy                           0.47      8232\n",
      "   macro avg       0.54      0.44      0.46      8232\n",
      "weighted avg       0.52      0.47      0.47      8232\n",
      "\n",
      "AdaBoost - Accuracy: 0.4530\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.36      0.44      1056\n",
      "           1       0.47      0.30      0.37      2006\n",
      "           2       0.45      0.55      0.49      1553\n",
      "           3       0.38      0.62      0.48      2287\n",
      "           4       0.69      0.36      0.48      1330\n",
      "\n",
      "    accuracy                           0.45      8232\n",
      "   macro avg       0.51      0.44      0.45      8232\n",
      "weighted avg       0.49      0.45      0.45      8232\n",
      "\n",
      "Gradient Boosting - Accuracy: 0.4944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.41      0.48      1056\n",
      "           1       0.51      0.36      0.43      2006\n",
      "           2       0.50      0.59      0.54      1553\n",
      "           3       0.42      0.62      0.50      2287\n",
      "           4       0.68      0.43      0.53      1330\n",
      "\n",
      "    accuracy                           0.49      8232\n",
      "   macro avg       0.54      0.48      0.50      8232\n",
      "weighted avg       0.52      0.49      0.49      8232\n",
      "\n",
      "XGBoost - Accuracy: 0.5694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.49      0.54      1056\n",
      "           1       0.54      0.47      0.50      2006\n",
      "           2       0.57      0.76      0.65      1553\n",
      "           3       0.52      0.59      0.55      2287\n",
      "           4       0.71      0.53      0.61      1330\n",
      "\n",
      "    accuracy                           0.57      8232\n",
      "   macro avg       0.59      0.57      0.57      8232\n",
      "weighted avg       0.58      0.57      0.57      8232\n",
      "\n",
      "LightGBM - Accuracy: 0.6149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.56      0.59      1056\n",
      "           1       0.57      0.50      0.53      2006\n",
      "           2       0.64      0.82      0.71      1553\n",
      "           3       0.59      0.59      0.59      2287\n",
      "           4       0.70      0.62      0.66      1330\n",
      "\n",
      "    accuracy                           0.61      8232\n",
      "   macro avg       0.62      0.62      0.62      8232\n",
      "weighted avg       0.61      0.61      0.61      8232\n",
      "\n",
      "CatBoost - Accuracy: 0.6011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.51      0.57      1056\n",
      "           1       0.56      0.52      0.54      2006\n",
      "           2       0.60      0.80      0.68      1553\n",
      "           3       0.56      0.62      0.59      2287\n",
      "           4       0.76      0.54      0.63      1330\n",
      "\n",
      "    accuracy                           0.60      8232\n",
      "   macro avg       0.63      0.60      0.60      8232\n",
      "weighted avg       0.61      0.60      0.60      8232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "df_train['Sentiment'] = df_train['Sentiment'].replace(['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'], [0, 1, 2, 3, 4])\n",
    "X = df_train['OriginalTweet'].values\n",
    "y = df_train['Sentiment'].values\n",
    "\n",
    "# Create a CountVectorizer representation of the data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "X = X.astype(np.float64)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# All classification models from scikit-learn\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('SVM', SVC(random_state=42)),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Naive Bayes', MultinomialNB()),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(eval_metric='mlogloss', random_state=42, use_label_encoder=False)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=42)),\n",
    "    ('CatBoost', CatBoostClassifier(random_state=42, verbose=0))\n",
    "]\n",
    "\n",
    "# Evaluate each model\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'{name} - Accuracy: {accuracy:.4f}')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "df_train = pd.read_csv(\"Corona_NLP_train.csv\", \n",
    "                       encoding='latin-1')\n",
    "df_test = pd.read_csv(\"Corona_NLP_test.csv\", \n",
    "                      encoding='latin-1')\n",
    "\n",
    "df_train = df_train[['OriginalTweet', 'Sentiment']]\n",
    "df_test = df_test[['OriginalTweet', 'Sentiment']]\n",
    "\n",
    "# clean data\n",
    "\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_emoji)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_all_entities)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(clean_hashtags)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(filter_chars)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(remove_mult_spaces)\n",
    "\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_emoji)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_all_entities)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(clean_hashtags)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(filter_chars)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(remove_mult_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.5424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.41      0.49      1056\n",
      "           1       0.48      0.49      0.49      2006\n",
      "           2       0.60      0.65      0.62      1553\n",
      "           3       0.49      0.59      0.54      2287\n",
      "           4       0.68      0.52      0.59      1330\n",
      "\n",
      "    accuracy                           0.54      8232\n",
      "   macro avg       0.57      0.53      0.54      8232\n",
      "weighted avg       0.55      0.54      0.54      8232\n",
      "\n",
      "K-Nearest Neighbors - Accuracy: 0.2440\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.08      0.14      1056\n",
      "           1       0.41      0.11      0.17      2006\n",
      "           2       0.20      0.88      0.33      1553\n",
      "           3       0.44      0.11      0.18      2287\n",
      "           4       0.50      0.06      0.11      1330\n",
      "\n",
      "    accuracy                           0.24      8232\n",
      "   macro avg       0.39      0.25      0.19      8232\n",
      "weighted avg       0.39      0.24      0.19      8232\n",
      "\n",
      "SVM - Accuracy: 0.5521\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.38      0.48      1056\n",
      "           1       0.48      0.53      0.50      2006\n",
      "           2       0.62      0.63      0.63      1553\n",
      "           3       0.49      0.65      0.56      2287\n",
      "           4       0.77      0.45      0.57      1330\n",
      "\n",
      "    accuracy                           0.55      8232\n",
      "   macro avg       0.60      0.53      0.55      8232\n",
      "weighted avg       0.58      0.55      0.55      8232\n",
      "\n",
      "Random Forest - Accuracy: 0.4900\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.30      0.41      1056\n",
      "           1       0.46      0.43      0.44      2006\n",
      "           2       0.56      0.60      0.58      1553\n",
      "           3       0.42      0.65      0.51      2287\n",
      "           4       0.68      0.34      0.45      1330\n",
      "\n",
      "    accuracy                           0.49      8232\n",
      "   macro avg       0.55      0.46      0.48      8232\n",
      "weighted avg       0.53      0.49      0.48      8232\n",
      "\n",
      "Decision Tree - Accuracy: 0.4094\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.35      0.37      1056\n",
      "           1       0.39      0.37      0.38      2006\n",
      "           2       0.46      0.54      0.50      1553\n",
      "           3       0.39      0.39      0.39      2287\n",
      "           4       0.43      0.40      0.41      1330\n",
      "\n",
      "    accuracy                           0.41      8232\n",
      "   macro avg       0.41      0.41      0.41      8232\n",
      "weighted avg       0.41      0.41      0.41      8232\n",
      "\n",
      "Naive Bayes - Accuracy: 0.3709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.01      0.01      1056\n",
      "           1       0.40      0.39      0.39      2006\n",
      "           2       0.74      0.13      0.22      1553\n",
      "           3       0.34      0.89      0.49      2287\n",
      "           4       0.84      0.02      0.05      1330\n",
      "\n",
      "    accuracy                           0.37      8232\n",
      "   macro avg       0.58      0.29      0.23      8232\n",
      "weighted avg       0.54      0.37      0.28      8232\n",
      "\n",
      "AdaBoost - Accuracy: 0.4288\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.34      0.42      1056\n",
      "           1       0.45      0.29      0.36      2006\n",
      "           2       0.49      0.33      0.40      1553\n",
      "           3       0.36      0.71      0.48      2287\n",
      "           4       0.65      0.33      0.44      1330\n",
      "\n",
      "    accuracy                           0.43      8232\n",
      "   macro avg       0.50      0.40      0.42      8232\n",
      "weighted avg       0.48      0.43      0.42      8232\n",
      "\n",
      "Gradient Boosting - Accuracy: 0.4710\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.39      0.45      1056\n",
      "           1       0.49      0.33      0.39      2006\n",
      "           2       0.52      0.53      0.52      1553\n",
      "           3       0.39      0.63      0.48      2287\n",
      "           4       0.64      0.42      0.50      1330\n",
      "\n",
      "    accuracy                           0.47      8232\n",
      "   macro avg       0.52      0.46      0.47      8232\n",
      "weighted avg       0.50      0.47      0.47      8232\n",
      "\n",
      "XGBoost - Accuracy: 0.5339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.45      0.49      1056\n",
      "           1       0.50      0.42      0.46      2006\n",
      "           2       0.57      0.68      0.62      1553\n",
      "           3       0.47      0.58      0.52      2287\n",
      "           4       0.67      0.52      0.58      1330\n",
      "\n",
      "    accuracy                           0.53      8232\n",
      "   macro avg       0.55      0.53      0.54      8232\n",
      "weighted avg       0.54      0.53      0.53      8232\n",
      "\n",
      "LightGBM - Accuracy: 0.5706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.54      0.56      1056\n",
      "           1       0.53      0.47      0.50      2006\n",
      "           2       0.62      0.72      0.67      1553\n",
      "           3       0.52      0.56      0.54      2287\n",
      "           4       0.64      0.60      0.62      1330\n",
      "\n",
      "    accuracy                           0.57      8232\n",
      "   macro avg       0.58      0.58      0.58      8232\n",
      "weighted avg       0.57      0.57      0.57      8232\n",
      "\n",
      "CatBoost - Accuracy: 0.5630\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.45      0.53      1056\n",
      "           1       0.52      0.50      0.51      2006\n",
      "           2       0.60      0.73      0.66      1553\n",
      "           3       0.51      0.59      0.55      2287\n",
      "           4       0.69      0.50      0.58      1330\n",
      "\n",
      "    accuracy                           0.56      8232\n",
      "   macro avg       0.59      0.56      0.56      8232\n",
      "weighted avg       0.57      0.56      0.56      8232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "df_train['Sentiment'] = df_train['Sentiment'].replace(['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'], [0, 1, 2, 3, 4])\n",
    "X = df_train['OriginalTweet'].values\n",
    "y = df_train['Sentiment'].values\n",
    "\n",
    "# Create a CountVectorizer representation of the data\n",
    "vectorizer = HashingVectorizer(n_features=2**14, alternate_sign=False)\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# All classification models from scikit-learn\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('SVM', SVC(random_state=42)),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Naive Bayes', MultinomialNB()),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(eval_metric='mlogloss', random_state=42, use_label_encoder=False)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=42)),\n",
    "    ('CatBoost', CatBoostClassifier(random_state=42, verbose=0))\n",
    "]\n",
    "\n",
    "# Evaluate each model\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'{name} - Accuracy: {accuracy:.4f}')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use `CountVectorizer` and Logistic Regression, LightGBM, XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "df_train = pd.read_csv(\"Corona_NLP_train.csv\", \n",
    "                       encoding='latin-1')\n",
    "df_test = pd.read_csv(\"Corona_NLP_test.csv\", \n",
    "                      encoding='latin-1')\n",
    "\n",
    "df_train = df_train[['OriginalTweet', 'Sentiment']]\n",
    "df_test = df_test[['OriginalTweet', 'Sentiment']]\n",
    "\n",
    "# clean data\n",
    "\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_emoji)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_all_entities)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(clean_hashtags)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(filter_chars)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(remove_mult_spaces)\n",
    "\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_emoji)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_all_entities)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(clean_hashtags)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(filter_chars)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(remove_mult_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Logistic Regression...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'solver': 'newton-cg', 'C': 0.615848211066026}\n",
      "Logistic Regression - Accuracy: 0.6277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.58      0.62      1056\n",
      "           1       0.56      0.55      0.56      2006\n",
      "           2       0.66      0.75      0.70      1553\n",
      "           3       0.60      0.62      0.61      2287\n",
      "           4       0.73      0.65      0.69      1330\n",
      "\n",
      "    accuracy                           0.63      8232\n",
      "   macro avg       0.64      0.63      0.63      8232\n",
      "weighted avg       0.63      0.63      0.63      8232\n",
      "\n",
      "\n",
      "\n",
      "Running LightGBM...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'num_leaves': 68, 'n_estimators': 70, 'max_depth': -1, 'learning_rate': 0.04832930238571752}\n",
      "LightGBM - Accuracy: 0.5948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.52      0.55      1056\n",
      "           1       0.55      0.50      0.52      2006\n",
      "           2       0.62      0.81      0.71      1553\n",
      "           3       0.57      0.59      0.58      2287\n",
      "           4       0.66      0.57      0.61      1330\n",
      "\n",
      "    accuracy                           0.59      8232\n",
      "   macro avg       0.60      0.60      0.59      8232\n",
      "weighted avg       0.59      0.59      0.59      8232\n",
      "\n",
      "\n",
      "\n",
      "Running XGBoost...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 11.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'subsample': 0.6, 'n_estimators': 180, 'min_child_weight': 1, 'max_depth': 6, 'learning_rate': 0.1, 'gamma': 2, 'eval_metric': 'mlogloss', 'colsample_bytree': 1.0}\n",
      "XGBoost - Accuracy: 0.5469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.45      0.52      1056\n",
      "           1       0.53      0.44      0.48      2006\n",
      "           2       0.54      0.71      0.62      1553\n",
      "           3       0.49      0.60      0.54      2287\n",
      "           4       0.71      0.49      0.58      1330\n",
      "\n",
      "    accuracy                           0.55      8232\n",
      "   macro avg       0.58      0.54      0.55      8232\n",
      "weighted avg       0.56      0.55      0.54      8232\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "df_train['Sentiment'] = df_train['Sentiment'].replace(['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'], [0, 1, 2, 3, 4])\n",
    "X = df_train['OriginalTweet'].values\n",
    "y = df_train['Sentiment'].values\n",
    "\n",
    "# Create a CountVectorizer representation of the data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "X = X.astype(np.float64)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {\n",
    "            'C': np.logspace(-4, 4, 20),\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(),\n",
    "        'params': {\n",
    "            'num_leaves': list(range(30, 150)),\n",
    "            'max_depth': [10, 20, 30, 40, 50, -1],\n",
    "            'learning_rate': np.logspace(-3, -1, 20),\n",
    "            'n_estimators': list(range(10, 101, 10))\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False),\n",
    "        'params': {\n",
    "            'learning_rate': np.logspace(-3, -1, 20),\n",
    "            'max_depth': list(range(3, 11)),\n",
    "            'n_estimators': list(range(50, 201, 10)),\n",
    "            'min_child_weight': [1, 5, 10],\n",
    "            'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'eval_metric': ['mlogloss']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning and model evaluation\n",
    "for name, model_info in models.items():\n",
    "    print(f\"Running {name}...\")\n",
    "    model = model_info['model']\n",
    "    params = model_info['params']\n",
    "    search = RandomizedSearchCV(model, params, cv=5, n_jobs=-1, \n",
    "                                n_iter=30, verbose=1, random_state=42)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = search.best_params_\n",
    "    \n",
    "    print(f\"Best parameters for {name}: {best_params}\")\n",
    "\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "df_train = pd.read_csv(\"Corona_NLP_train.csv\", \n",
    "                       encoding='latin-1')\n",
    "df_test = pd.read_csv(\"Corona_NLP_test.csv\", \n",
    "                      encoding='latin-1')\n",
    "\n",
    "df_train = df_train[['OriginalTweet', 'Sentiment']]\n",
    "df_test = df_test[['OriginalTweet', 'Sentiment']]\n",
    "\n",
    "# clean data\n",
    "\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_emoji)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_all_entities)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(clean_hashtags)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(filter_chars)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(remove_mult_spaces)\n",
    "\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_emoji)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_all_entities)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(clean_hashtags)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(filter_chars)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(remove_mult_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LR Params: {'C': 1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best LR Score: 0.6477820125418517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "df_train['Sentiment'] = df_train['Sentiment'].replace(['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'], [0, 1, 2, 3, 4])\n",
    "X = df_train['OriginalTweet'].values\n",
    "y = df_train['Sentiment'].values\n",
    "\n",
    "# Create a CountVectorizer representation of the data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "X = X.astype(np.float64)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression tuning\n",
    "lr_params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'max_iter': [50, 100, 500, 1000]\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(LogisticRegression(random_state=42), lr_params, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best LR Params:', lr_grid.best_params_)\n",
    "print('Best LR Score:', lr_grid.best_score_)\n",
    "\n",
    "# LightGBM tuning\n",
    "lgbm_params = {\n",
    "    'n_estimators': [100, 300, 500, 1000],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.3],\n",
    "    'num_leaves': [31, 50, 100, 150],\n",
    "    'min_child_samples': [5, 20, 50, 100],\n",
    "    'subsample': [0.5, 0.8, 1],\n",
    "    'colsample_bytree': [0.5, 0.8, 1],\n",
    "    'max_depth': [-1, 3, 6, 9],\n",
    "    'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "    'min_split_gain': [0, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "lgbm_grid = GridSearchCV(LGBMClassifier(random_state=42), lgbm_params, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "lgbm_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best LGBM Params:', lgbm_grid.best_params_)\n",
    "print('Best LGBM Score:', lgbm_grid.best_score_)\n",
    "\n",
    "# XGBoost tuning\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 300, 500, 1000],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 6, 9, 12, 15],\n",
    "    'min_child_weight': [1, 5, 10, 20],\n",
    "    'subsample': [0.5, 0.8, 1],\n",
    "    'colsample_bytree': [0.5, 0.8, 1],\n",
    "    'gamma': [0, 0.1, 0.3, 0.5, 1],\n",
    "    'reg_alpha': [0, 1e-5, 1e-2, 0.1, 1, 100],\n",
    "    'reg_lambda': [0, 1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "print('Best XGBoost Parameters:', xgb_grid.best_params_)\n",
    "\n",
    "xgb_grid = GridSearchCV(XGBClassifier(random_state=42), xgb_params, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best XGB Params:', xgb_grid.best_params_)\n",
    "print('Best XGB Score:', xgb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "df_train = pd.read_csv(\"Corona_NLP_train.csv\", \n",
    "                       encoding='latin-1')\n",
    "df_test = pd.read_csv(\"Corona_NLP_test.csv\", \n",
    "                      encoding='latin-1')\n",
    "\n",
    "df_train = df_train[['OriginalTweet', 'Sentiment']]\n",
    "df_test = df_test[['OriginalTweet', 'Sentiment']]\n",
    "\n",
    "# clean data\n",
    "\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_emoji)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(strip_all_entities)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(clean_hashtags)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(filter_chars)\n",
    "df_train['OriginalTweet'] = df_train['OriginalTweet'].apply(remove_mult_spaces)\n",
    "\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_emoji)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(strip_all_entities)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(clean_hashtags)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(filter_chars)\n",
    "df_test['OriginalTweet'] = df_test['OriginalTweet'].apply(remove_mult_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and and</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus australia woolworths to give elder...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>me ready to go at supermarket during the covid...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>airline pilots offering to stock supermarket s...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>response to complaint not provided citing covi...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>you know its getting tough when is rationing t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>is it wrong that the smell of hand sanitizer i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>well newused rift s are going for 70000 on ama...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           OriginalTweet           Sentiment\n",
       "0                                                and and             Neutral\n",
       "1      advice talk to your neighbours family to excha...            Positive\n",
       "2      coronavirus australia woolworths to give elder...            Positive\n",
       "3      my food stock is not the only one which is emp...            Positive\n",
       "4      me ready to go at supermarket during the covid...  Extremely Negative\n",
       "...                                                  ...                 ...\n",
       "41152  airline pilots offering to stock supermarket s...             Neutral\n",
       "41153  response to complaint not provided citing covi...  Extremely Negative\n",
       "41154  you know its getting tough when is rationing t...            Positive\n",
       "41155  is it wrong that the smell of hand sanitizer i...             Neutral\n",
       "41156  well newused rift s are going for 70000 on ama...            Negative\n",
       "\n",
       "[41157 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Sentiment']=df_train['Sentiment'].replace({'Neutral':2, 'Positive':3,'Extremely Positive':4, 'Extremely Negative':0,'Negative':1})\n",
    "df_train['Sentiment']=df_train['Sentiment'].astype(int)\n",
    "df_train['tweet_length'] = df_train['OriginalTweet'].apply(lambda t: len(t.split()))\n",
    "\n",
    "df_test['Sentiment']=df_test['Sentiment'].replace({'Neutral':2, 'Positive':3,'Extremely Positive':4, 'Extremely Negative':0,'Negative':1})\n",
    "df_test['Sentiment']=df_test['Sentiment'].astype(int)\n",
    "df_test['tweet_length'] = df_test['OriginalTweet'].apply(lambda t: len(t.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.rename(columns = {'OriginalTweet': 'text', 'Sentiment': 'label'})\n",
    "df_test = df_test.rename(columns = {'OriginalTweet': 'text', 'Sentiment': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_train_data = Dataset.from_pandas(df_train)\n",
    "hg_test_data = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'advice talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist gp set up online shopping accounts if poss adequate supplies of regular meds but not over order',\n",
       " 'label': 3,\n",
       " 'tweet_length': 38}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg_train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer from a pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Take a look at the tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'and and', 'label': 2, 'tweet_length': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1eb32270de64939a3ec477ba87478f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7cb4891dfe14960973bd8e5b7bb38c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funtion to tokenize data\n",
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data[\"text\"], \n",
    "                     max_length=128, \n",
    "                     truncation=True, \n",
    "                     padding=\"max_length\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "dataset_train = hg_train_data.map(tokenize_dataset)\n",
    "dataset_test = hg_test_data.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'tweet_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 41157\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'tweet_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 3798\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "print(dataset_train)\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/\",          \n",
    "    logging_dir='./results/logs',            \n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=100,    \n",
    "    num_train_epochs=10,              \n",
    "    per_device_train_batch_size=32,  \n",
    "    per_device_eval_batch_size=2,  \n",
    "    learning_rate=5e-6,\n",
    "    seed=42,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the metric\n",
    "# def compute_metrics(eval_pred):\n",
    "#     metric = evaluate.load(\"f1\", average=\"weighted\")\n",
    "#     logits, labels = eval_pred\n",
    "#     # probabilities = tf.nn.softmax(logits)\n",
    "#     predictions = np.argmax(logits, axis=1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # Choose the appropriate average setting for your multi-class problem\n",
    "    average_setting = 'weighted'  # You can use 'micro', 'macro', or 'weighted'\n",
    "    \n",
    "    # Compute the F1 score\n",
    "    f1 = f1_score(labels, predictions, average=average_setting)\n",
    "    \n",
    "    # Return the F1 score as a dictionary\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1856' max='3220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1856/3220 18:19 < 13:29, 1.69 it/s, Epoch 5.76/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.376600</td>\n",
       "      <td>1.098551</td>\n",
       "      <td>0.541960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.950600</td>\n",
       "      <td>0.896920</td>\n",
       "      <td>0.638786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.786300</td>\n",
       "      <td>0.799974</td>\n",
       "      <td>0.688557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>0.742104</td>\n",
       "      <td>0.719266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.629600</td>\n",
       "      <td>0.719007</td>\n",
       "      <td>0.741891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(df, textCol, labelCol):\n",
    "    dataset_dict = {\n",
    "        'text' : df[textCol],\n",
    "        'labels' : df[labelCol],\n",
    "      }\n",
    "    sent_tags = ClassLabel(num_classes=5 , names=['Extremely Negative', 'Negative','Neutral','Positive', 'Extremely Positive'])\n",
    "\n",
    "    return Dataset.from_dict(\n",
    "        mapping = dataset_dict,\n",
    "        features = Features({'text' : Value(dtype='string') , 'labels' :sent_tags})\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tweet_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and and</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice talk to your neighbours family to excha...</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus australia woolworths to give elder...</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my food stock is not the only one which is emp...</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>me ready to go at supermarket during the covid...</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>airline pilots offering to stock supermarket s...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>response to complaint not provided citing covi...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>you know its getting tough when is rationing t...</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>is it wrong that the smell of hand sanitizer i...</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>well newused rift s are going for 70000 on ama...</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41157 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           OriginalTweet  Sentiment  \\\n",
       "0                                                and and          2   \n",
       "1      advice talk to your neighbours family to excha...          3   \n",
       "2      coronavirus australia woolworths to give elder...          3   \n",
       "3      my food stock is not the only one which is emp...          3   \n",
       "4      me ready to go at supermarket during the covid...          0   \n",
       "...                                                  ...        ...   \n",
       "41152  airline pilots offering to stock supermarket s...          2   \n",
       "41153  response to complaint not provided citing covi...          0   \n",
       "41154  you know its getting tough when is rationing t...          3   \n",
       "41155  is it wrong that the smell of hand sanitizer i...          2   \n",
       "41156  well newused rift s are going for 70000 on ama...          1   \n",
       "\n",
       "       tweet_length  \n",
       "0                 2  \n",
       "1                38  \n",
       "2                13  \n",
       "3                41  \n",
       "4                39  \n",
       "...             ...  \n",
       "41152            11  \n",
       "41153            22  \n",
       "41154            16  \n",
       "41155            18  \n",
       "41156            44  \n",
       "\n",
       "[41157 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_train.sample(frac = 0.1, random_state=42)\n",
    "\n",
    "df_train = df_train.drop(df_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 37041\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 4116\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 3798\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = createDataset(df_train,\"OriginalTweet\",\"Sentiment\")\n",
    "dataset_val = createDataset(df_val,\"OriginalTweet\",\"Sentiment\")\n",
    "dataset_test = createDataset(df_test,\"OriginalTweet\",\"Sentiment\")\n",
    "\n",
    "dataset_sentAnalysis = DatasetDict()\n",
    "dataset_sentAnalysis[\"train\"] = dataset_train\n",
    "dataset_sentAnalysis[\"val\"] = dataset_val\n",
    "dataset_sentAnalysis[\"test\"] = dataset_test\n",
    "\n",
    "dataset_sentAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4d11929a4543fc8828c548d00142e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca95ee4041ba42e0a7bc58f53b2fc08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e05d86344a4871ab3274eca2b3facf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f4618df45a4acb873b7de740e6501d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 ['What', 'is', 'going', 'on', '@', 'rest', '##ura', '##nt', '.', ':', '(', 'It', 'makes', 'Me', 'Feel', 'Up', '##set', '.', '[UNK]']\n",
      "19 [1327, 1110, 1280, 1113, 137, 1832, 4084, 2227, 119, 131, 113, 1135, 2228, 2508, 14425, 3725, 9388, 119, 100]\n",
      "{'input_ids': [101, 1327, 1110, 1280, 1113, 137, 1832, 4084, 2227, 119, 131, 113, 1135, 2228, 2508, 14425, 3725, 9388, 119, 100, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "21\n",
      "['[CLS]', 'What', 'is', 'going', 'on', '@', 'rest', '##ura', '##nt', '.', ':', '(', 'It', 'makes', 'Me', 'Feel', 'Up', '##set', '.', '[UNK]', '[SEP]']\n",
      "[101, 1327, 1110, 1280, 1113, 137, 1832, 4084, 2227, 119, 131, 113, 1135, 2228, 2508, 14425, 3725, 9388, 119, 100, 102]\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"What is  going on @resturant.:( It makes   Me Feel Upset.😞\"\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(len(tokens), tokens)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(len(token_ids), token_ids)\n",
    "\n",
    "token_dictionary = tokenizer(sample_text)\n",
    "print(token_dictionary)\n",
    "print(len(token_dictionary.input_ids)) # automatically added cls, sep\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_dictionary['input_ids'])\n",
    "print(tokens)\n",
    "\n",
    "token_encode = tokenizer.encode(sample_text) #convert to tokens ids but with cls+sep\n",
    "print(token_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "[SEP] 102\n",
      "[CLS] 101\n",
      "[PAD] 0\n",
      "[UNK] 100\n"
     ]
    }
   ],
   "source": [
    "# we use special tokens to separate the sentences. How bert works\n",
    "print(tokenizer)\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id)\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1327,  1110,  1280,  1113,   137,  1832,  4084,  2227,   119,\n",
       "           131,   113,  1135,  2228,  2508, 14425,  3725,  9388,   119,   100,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the inputs same as distilbert_tokenizer()\n",
    "encoding = tokenizer.encode_plus(\n",
    "    sample_text,\n",
    "    max_length=max_len,\n",
    "    add_special_tokens=True,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d40a5889a949f0a437e3ba51d20f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88dacb61b8b141d494fd3ee9ad1f65ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba85fd08e0b4fffb56d7c62964c53be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_sentAnalysis_encoded = dataset_sentAnalysis.map(tokenize, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 37041\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4116\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3798\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sentAnalysis_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertForClassification(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        # Load model body > return all og the HS\n",
    "        self.bert = BertModel(config)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n",
    "                labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        # Apply classifier to encoder representation > [cls]\n",
    "        sequence_output = self.dropout(outputs[1])\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        # Return model output object\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "id2label = {\n",
    "    0: 'Extremely Negative',\n",
    "    1: 'Negative',\n",
    "    2: 'Neutral',\n",
    "    3: 'Positive',\n",
    "    4: 'Extremely Positive'\n",
    "}\n",
    "\n",
    "label2id = { v:k for (k,v) in id2label.items()}\n",
    "\n",
    "bert_config = AutoConfig.from_pretrained(\"bert-base-cased\", \n",
    "                                         num_labels=5,\n",
    "                                         id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71515eaa3b04368bc66a50809387b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = (BertForClassification\n",
    "              .from_pretrained(\"bert-base-cased\", config=bert_config)\n",
    "              .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "\n",
    "# wandb.init(project=\"bert-for-english-classification\")\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = len(dataset_sentAnalysis_encoded[\"train\"]) // batch_size\n",
    "model_name = \"bert-base-cased-finetuned-sentimentAnalysis-bert\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  save_steps=1e6,\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=False, \n",
    "                                  log_level=\"error\",\n",
    "#                                   report_to=\"wandb\",\n",
    "                                  run_name=\"bert-sent-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/transformers/integrations.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tb_writer)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__version__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1.15'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-4e86eae9a42e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                   \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_sentAnalysis_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                   \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                   optimizers=(optimizer,lr_scheduler))\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtrainer_preprocessed_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_callbacks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdefault_callbacks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         self.callback_handler = CallbackHandler(\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         )\n\u001b[1;32m    522\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPrinterCallback\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_tqdm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDEFAULT_PROGRESS_CALLBACK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36madd_callback\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mcb_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_class\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work2/09059/xliaoyi/frontera/software/anaconda3/lib/python3.7/site-packages/transformers/integrations.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tb_writer)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SummaryWriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorboardX/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecord_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecordWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorchvis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTorchVis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorboardX/torchvis.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvisdom_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisdomWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorboardX/writer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_sprite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_tsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend_pbtxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevent_file_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEventFileWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0monnx_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpytorch_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorboardX/event_file_writer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecord_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecordWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorboardX/proto/event_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboardX_dot_proto_dot_summary__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorboardX/proto/summary_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboardX_dot_proto_dot_tensor__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorboardX/proto/tensor_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_handle_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboardX_dot_proto_dot_resource__handle__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboardX_dot_proto_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboardX_dot_proto_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorboardX/proto/resource_handle_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mmessage_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menum_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontaining_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mis_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       serialized_options=None, file=DESCRIPTOR),\n\u001b[0m\u001b[1;32m     42\u001b[0m     _descriptor.FieldDescriptor(\n\u001b[1;32m     43\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'container'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tensorboardX.ResourceHandleProto.container'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * logging_steps\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "trainer_preprocessed_lr = Trainer(model=bert_model, args=training_args,\n",
    "                                  compute_metrics=compute_metrics,\n",
    "                                  train_dataset=dataset_sentAnalysis_encoded[\"train\"],\n",
    "                                  eval_dataset=dataset_sentAnalysis_encoded[\"val\"],\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  optimizers=(optimizer,lr_scheduler))\n",
    "\n",
    "trainer_preprocessed_lr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
